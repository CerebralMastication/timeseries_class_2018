---
title: "Time Series Notes"
author: "JD Long"
date: "6/26/2018"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=TRUE, messages=FALSE, warnings=FALSE)
```

## Intro

Rob Hyndman from Monash University has been teaching and researching time series analysis for 20 years. He is the author of the R package `forcast` (https://github.com/robjhyndman/forecast). This package has long been a staple for time series analysis in R. His latest book is called *Forecasting: Principles and Practice* and is available online at https://otexts.org/fpp2/. The book has an accompanying package called `fpp2` which has example code and data from the book. 

Load up the needed packages:

```{r, message=FALSE}
library(fpp2)
library(ggplot2)
library(magrittr)
```

## Day One
Exercise data is here: https://robjhyndman.com/seminars/NYC2018/

The first day was spent incrementally working through the plotting and modeling methods in `forecast`.  Examples were done building up each type of time series model. Then we looked at time series cross validation methods. The culmination of the day was learning the `ets` model which is an autofitted model that goes through a number of combinations of models and automagically picks the model that fits the data the best. 

### ETS example:
```{r}
## example data
retaildata <- read.csv("https://robjhyndman.com/nyc2018/retail.csv")
mytimeseries <- ts(retaildata[,4], frequency=12, start=c(1982,4))

ets(mytimeseries)
ets(mytimeseries) %>% autoplot()
ets(mytimeseries) %>% forecast() %>% autoplot()
```

```{r}
USAccDeaths %>% 
  ets %>% 
  forecast ->
forecasted_stuff 

head(forecasted_stuff)

forecasted_stuff %>%
  autoplot
```

## Day 2

Transformations using Box Cox

### Multiple Seasonal Decomposition

```{r}
elecequip %>%
  mstl %>%
  autoplot
```

worked with the components:

* `t.window` controls wiggliness of trend component.
* `s.window` controls variation on seasonal component.
* `seasonal()` extracts seasonal component
* `trendcycle()` extracts trend component
* `remainder()` extracts remainder component
* `seasadj()` computes seasonally adjusted data

Built up incrementally and then looked at residuals:

```{r}
bricksq %>%
  ets( ) ->
  fitted_stuff

checkresiduals(fitted_stuff)
autoplot(fitted_stuff)
accuracy(fitted_stuff)
```

## ARIMA
https://otexts.org/fpp2/arima.html

ETS models do not depend on being stationary?
ARIMA models do require stationarity. 

ACF for stationary data drops to zero quickly (faster than linear). 

This is NOT stationary:
```{r}
autoplot(dj) + ylab("Dow Jones Index") + xlab("Day")
ggAcf(dj)

```
 
 But a simple `diff` will make it stationary:
 
 ```{r}
autoplot(diff(dj)) +
ylab("Change in Dow Jones Index") + xlab("Day")
ggAcf(diff(dj))
```

for non stationary data r1 is often large and positive

Discussion of KPSS  & Augmented Dickey Fuller tests. KPSS and Dickey Fuller have reverse null hypothesis, because they like messing with us, I presume. 

```{r}
library(urca)
summary(ur.kpss(goog))

## forecast package has this helper function to make all this less painful
## only works for non seasonal data. 
ndiffs(goog)
```

`auto.arima` takes care of the lags, but it does not do the Box Cox transform. So in practice just transform it, then `auto.arima` or `ets` then go have a sammich. 

```{r}

a10 %>%
  BoxCox.lambda() ->
  lambda

a10 %>%
  BoxCox(lambda) %>%
  auto.arima() ->
  fit

print(fit)
checkresiduals(fit, plot=TRUE)
fit %>% forecast() %>% autoplot()

```

## Day 3:
### Regression with ARIMA errors

Can't bring regression into ETS, must use ARIMA. 

Rob encourages avoiding ARMIAx models as the coefficients are not inheritable. Stick to modeling the error as an ARIMA. 

Could do an `lm`, grab errors, then `auto.arima` on the errors. But that gives other struggles. So we do them at once:

Use MLE method by adding regression parameters to `auto.arima` Have to make categorical variables into dummies. 

TBATS is a whole bag of tricks. Point forecasts are pretty good but conf bands can be nuts. Does not support covariates. 

## Hierarchy Modeling: 
forecast it all at all levels of aggregation
reconcile results
use `hts` package 

```{r, fig.height=8, fig.width=10}
library(hts)
plot(infantgts)
# smatrix(infantgts)

# Forecast 10-steps-ahead and reconcile the forecasts
infantforecast <- forecast(infantgts, h=10)

# Plot the forecasts including only the last ten historical years
plot(infantforecast, include=10)

# set up training and testing sets
training <- window(infantgts, end=1993)
test <- window(infantgts, start=1994)

# Compute forecasts on training data
forecast <- forecast(training, h=10)

# calculate ME, RMSE, MAE, MAPE, MPE and MASE
accuracy.gts(forecast, test)

```


## Fourier Transform example
straight out of the text book. Example of searching for the best K in the Fourier:
```{r, fig.height=7, fig.width=10}
cafe04 <- window(auscafe, start=2004)
plots <- list()
for (i in seq(6)) {
  fit <- auto.arima(cafe04, xreg = fourier(cafe04, K = i),
    seasonal = FALSE, lambda = 0)
  plots[[i]] <- autoplot(forecast(fit,
      xreg=fourier(cafe04, K=i, h=24))) +
    xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) +
    ylab("") + ylim(1.5,4.7)
}
gridExtra::grid.arrange(
  plots[[1]],plots[[2]],plots[[3]],
  plots[[4]],plots[[5]],plots[[6]], nrow=3)
```

