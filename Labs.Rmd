---
title: "Lab Sessions: Forecasting using R"
author: "JD Long"
date: "25 June 2018"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=TRUE, messages=FALSE, warnings=FALSE)
library(fpp2)
library(ggplot2)
library(magrittr)
#install.packages('here')
```

textbook can be found: https://otexts.org/fpp2/

# ts objects
- annual freq=1, start=1995
- quarterly freq=4, start=c(1995, 2)
- monthly freq=12, start=c(1995, 9)
- daily freq=(7 or 365.25), start=(1 or c(1995, 234))
- weekly freq=52.18, start=c(1995, 23)
- hourly freq=(24 or 168 or 8766), start=1
- half-hourly freq=(48 or...)

```{r}
retaildata <- read.csv("https://robjhyndman.com/nyc2018/retail.csv")
mytimeseries <- ts(retaildata[,4], frequency=12, start=c(1982,4))
```


```{r}
autoplot(a10)
ggseasonplot(a10)
ggseasonplot(a10, polar=TRUE) + ylab("$ million")
ggsubseriesplot(a10)
```

```{r}
beer <- window(ausbeer, start=1992)
autoplot(beer)
ggseasonplot(beer)
ggsubseriesplot(beer)
```

# Lab Session 1

```{r}
autoplot(mytimeseries)
ggseasonplot(mytimeseries)
ggseasonplot(mytimeseries, polar=TRUE) + ylab("$ million")
ggsubseriesplot(mytimeseries)
```

# Lab Session 2

Seasonality is fixed length, cyclic data have variables length


```{r}
gglagplot(beer, lags=9)
ggAcf(beer)
```

```{r}
elec2 <- window(elec, start=1980)
ggAcf(elec2, lag.max=48)
```

- More correction every 12 months
- All lags are positive showing upward trend

```{r}
autoplot(goog)
ggAcf(goog, lag.max=100)
```

- no seasonality
- everything is positive

# Session Material

```{r}
wn <- ts(rnorm(36))
autoplot(wn)
ggAcf(wn)
```

```{r}
pigs2 <- window(pigs, start=1990)
autoplot(pigs2)
gglagplot(pigs2)
ggAcf(pigs2)
ggseasonplot(pigs2)
ggsubseriesplot(pigs2)
```

# Lab Session 3

```{r}
autoplot(bicoal)
gglagplot(bicoal)
ggAcf(bicoal)
```

```{r}
autoplot(chicken)
gglagplot(chicken)
ggAcf(chicken)
```

```{r}
autoplot(bricksq)
gglagplot(bricksq)
ggAcf(bricksq)
ggseasonplot(bricksq)
ggsubseriesplot(bricksq)
```

# Forecasting Benchmarks

- meanf: simply take the mean
- naive: use last value
- snaive: seasonal naive, take the value of the last same season
- rwf: random walk forecast, use drift

```{r}
meanf(goog, h=20)
naive(goog, h=20)
snaive(goog, h=20)
rwf(goog, drift=TRUE, h=50)

goog %>% meanf(h=20) %>% autoplot
goog %>% naive(h=20) %>% autoplot
goog %>% snaive(h=20) %>% autoplot
goog %>% rwf(drift=TRUE, h=50) %>% autoplot
```

# Forecasting Residuals

If the fitted values aren't good, then 

```{r}
goog %>% rwf(drift=TRUE, h=500) %>% fitted() -> z
autoplot(goog, series='Data') + autolayer(z, series='Fitted')
```

Residuals $e_t$ should not be correlated, if they are then information left in the residuals shoudl be used in the forecast

If the mean is not 0 then the forecast is biased

```{r}
fits <- fitted(naive(goog200))
autoplot(goog200, series='Data') + autolayer(fits, series='Fitted')
res <- residuals(naive(goog200))
autoplot(res)
gghistogram(res, add.normal=TRUE)
ggAcf(res)
checkresiduals(naive(goog200))
```

`checkresiduals` also computes the Ljung-Box test to make sure the residuals are white noise, if p-value is large it is white noise


# Lab Session 4

```{r}
beer <- window(ausbeer, start=1992)
fc <- snaive(beer)
autoplot(fc)
res <- residuals(fc)
autoplot(res)
checkresiduals(fc)
```

This tiny p-value means it's not white noise and we can do better.

# Evaluating Forecast Accuracy

$$
    MAPE = 100mean(|e_{T+h}|/y_{T+h})
$$

MAPE doesn't work well when numbers are close to 0 because of division by $y_{T+h}$

MAPE also doesn't make sense when there is no natura 0, like temperature

Instead use Mean Absolute Scaled Error

$$
    MASE = T^{-1} \sum_{t=1}^T |y_t \hat{y_{t|t-1}}| / Q
$$
 
where Q is a stable measure of the scale of the time series
 
 
```{r}
googtrain <- window(goog200, end=180)

googfc1 <- meanf(googtrain, h=20)
googfc2 <- naive(googtrain, h=20)
googfc3 <- rwf(googtrain, drift=TRUE, h=20)

accuracy(googfc1, goog200)
accuracy(googfc2, goog200)
accuracy(googfc3, goog200)
```
 
# Lab Session 5

```{r}
train <- window(mytimeseries, end=c(2010, 12))
test <- window(mytimeseries, start=2011)
```

```{r}
autoplot(cbind(Training=train, Test=test))
```

```{r}
fcst1 <- snaive(train, h=length(test))
```

```{r}
accuracy(fcst1, test)
checkresiduals(fcst1)
fcst1 %>% autoplot() + autolayer(test)
```

# Time Series Cross-Validation

Rob coined the phrase

rolling training data, all starting at the same point

```{r}
e <- tsCV(goog200, rwf, drift=TRUE, h=1)
sqrt(mean(e^2, na.rm=TRUE))
```

Now with pipes

```{r}
e <- goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1)
e^2 %>% mean(na.rm=TRUE) %>% sqrt()
```

Now get error for h=1, h=2, h=3, etc

```{r}
e <- goog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=12)
e^2 %>% colMeans(na.rm=TRUE) %>% sqrt()
```

# Lab Session 6

```{r}
e1 <- mytimeseries %>% tsCV(forecastfunction=rwf, drift=TRUE, h=12)
e2 <- mytimeseries %>% tsCV(forecastfunction=meanf, h=12)
e3 <- mytimeseries %>% tsCV(forecastfunction=naive, h=12)
e4 <- mytimeseries %>% tsCV(forecastfunction=snaive, h=12)

e1^2 %>% colMeans(na.rm=TRUE) %>% sqrt()
e2^2 %>% colMeans(na.rm=TRUE) %>% sqrt()
e3^2 %>% colMeans(na.rm=TRUE) %>% sqrt()
e4^2 %>% colMeans(na.rm=TRUE) %>% sqrt()
```

For one-ahead, this gives the overall error for each method

```{r}
e1 <- mytimeseries %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1)
e2 <- mytimeseries %>% tsCV(forecastfunction=meanf, h=1)
e3 <- mytimeseries %>% tsCV(forecastfunction=naive, h=1)
e4 <- mytimeseries %>% tsCV(forecastfunction=snaive, h=1)

e1^2 %>% mean(na.rm=TRUE) %>% sqrt()
e2^2 %>% mean(na.rm=TRUE) %>% sqrt()
e3^2 %>% mean(na.rm=TRUE) %>% sqrt()
e4^2 %>% mean(na.rm=TRUE) %>% sqrt()

cbind(RWF=e1^2, Mean=e2^2, Naive=e3^2, SNaive=e4^2) %>% na.omit() %>% colMeans()
```

# Exponential Smoothing

## Simple Exponential Smoothing

Robert Goodall Brown

```{r}
fc <- ses(oil, h=12)
autoplot(fc)
summary(fc)
summary(fc$model)
```

## Holt's Linear Trend

Charles Holt (US Navy) (1957)

```{r}
window(ausair, start=1990, end=2004) %>% 
    holt(h=5, PI=FALSE) %>% 
    summary()

window(ausair, start=1990, end=2004) %>% 
    holt(h=5, PI=FALSE) %>% 
    autoplot()
```

## Damped Trend Method

1980s: Gardner (Houston) & McKinsey (Scotland)

```{r}
autoplot(livestock)
livestock2 <- livestock %>% window(start=1970, end=2000)
livestock2 %>% holt(h=20, damped=TRUE) %>% autoplot()  
```

Beta=0.3 is pretty big

phi=0.8 is artifically constrained to be the smallest phi can be

# Lab Session 7

SES for non-trended, Holt for trended

```{r}
eggs2 <- window(eggs, start=1900, end=1993)

fc1 <- ses(eggs2, h=100)
fc2 <- holt(eggs2, h=100)
fc3 <- holt(eggs2, h=100, damped=TRUE)
```

```{r}
fc1 %>% autoplot()
fc2 %>% autoplot()
fc3 %>% autoplot()

fc1 %>% autoplot(PI=FALSE)
fc2 %>% autoplot(PI=FALSE)
fc3 %>% autoplot(PI=FALSE)
```

```{r}
residuals(fc1)^2 %>% mean() %>% sqrt()
residuals(fc2)^2 %>% mean() %>% sqrt()
residuals(fc3)^2 %>% mean() %>% sqrt()
```

```{r}
fc1 %>% accuracy()
fc2 %>% accuracy()
fc3 %>% accuracy()

list(SES=fc1, Trend=fc2, DampedTrend=fc3) %>% purrr::map_df(~ accuracy(.x) %>% as.data.frame(), .id='Model')
```

- if you care about mean, use RMSE
- if you care about median, use MAE or MASE

```{r}
fc1 %>% checkresiduals()
fc2 %>% checkresiduals()
fc3 %>% checkresiduals()
```

# Seasonal Methods

Holt Winters: Holt wrote the pape in 1957, separately Winters (Holt's students) wrote the code 3 years later

```{r}
aust <- window(austourists, start=2005)
fc1 <- hw(aust, seasonal='additive')
fc2 <- hw(aust, seasonal='multiplicative')
```

```{r}
fc1 %>% autoplot()
fc2 %>% autoplot()
```

```{r}
fc1 %>% summary()
fc2 %>% summary()
```

```{r}
fc1 %>% checkresiduals()
fc2 %>% checkresiduals()
```

```{r}
fc1$model %>% autoplot()
fc2$model %>% autoplot()
```


## Holt Winters with Dampening

F Gardner uses mostly this

So does Rob if he can only use 1 tool

If you only have 1 tool this is it

# Lab Session 8

```{r}
fc1 <- mytimeseries %>% hw(h=12, seasonal='multiplicative', damped=FALSE)
fc2 <- mytimeseries %>% hw(h=12, seasonal='multiplicative', damped=TRUE)
```

```{r}
fc1 %>% autoplot()
fc2 %>% autoplot()
```

```{r}
fc1$model %>% autoplot()
fc2$model %>% autoplot()
```

```{r}
fc1 %>% summary()
fc2 %>% summary()
```

```{r}
fc1 %>% checkresiduals()
fc2 %>% checkresiduals()
```

```{r}
fc1 %>% accuracy()
fc2 %>% accuracy()
```

# ETS (exponential smoothing)
Exponential smoothing methods:
* return point forecasts

Innovations state space models:
* general pobabalistic stuff

estimating ETS models:

```{r}
ets(mytimeseries)
```

```{r}
ets(mytimeseries) %>% autoplot()
```


```{r}
ets(mytimeseries) %>% forecast() %>% autoplot()
```


```{r}
USAccDeaths %>% 
  ets %>% 
  forecast ->
forecasted_stuff 

head(forecasted_stuff)

forecasted_stuff %>%
  autoplot
```


```{r}
h02 %>% ets %>% forecast %>% autoplot

h02 %>% ets %>% print
```

```{r}

autoplot(a10)

a10 %>% ets %>% autoplot
```


```{r}

jdl_model <- function(dataset) {
  dataset %>% ets -> fit_ets
  print(fit_ets)
  print(fit_ets %>% autoplot)
  print(fit_ets %>% forecast %>% autoplot)
}

jdl_model(bicoal)
jdl_model(chicken)
jdl_model(dole)
jdl_model(usdeaths)
jdl_model(bricksq)
jdl_model(lynx)
jdl_model(ibmclose)
jdl_model(eggs)
jdl_model(ausbeer)

```


```{r, eval=FALSE}
## works but needs to print name on every run of the function

data_list <- list(bicoal, chicken, dole, usdeaths, bricksq, lynx, ibmclose, eggs, ausbeer)
data_list$names <- c("bicoal", "chicken", "dole", "usdeaths", "bricksq", "lynx", "ibmclose", "eggs", "ausbeer")
lapply(data_list, jdl_model)
```



```{r}
autoplot(mytimeseries)

train <- window(mytimeseries, end=c(2010,12))
test  <- window(mytimeseries, start=2011)

f1 <- snaive(train, h=length(test))
f2 <- rwf(train, h=length(test))
f3 <- rwf(train, drift = TRUE, h=length(test))
f4 <- meanf(train, h=length(test))
f5 <- hw(train, h=length(test), seasonal = 'multiplicative')
f6 <- ets(train) %>% forecast(h=length(test))

c(
  SN=accuracy(f1, test)[2,"RMSE"],
  rwf1=accuracy(f2, test)[2,"RMSE"],
  rwf2=accuracy(f3, test)[2,"RMSE"],
  meanf=accuracy(f4, test)[2,"RMSE"],
  hw=accuracy(f5, test)[2,"RMSE"],
  ets=accuracy(f6, test)[2,"RMSE"]
)
```

now let's do time series cross validation 

```{r}
e1 <- tsCV(mytimeseries, snaive, h=12)
e2 <- tsCV(mytimeseries, naive, h=12)
e3 <- tsCV(mytimeseries, rwf, drift=TRUE, h=12)
e4 <- tsCV(mytimeseries, meanf, h=12)
e5 <- tsCV(mytimeseries, hw, h=12, seasonal='multiplicative')

etsfc <- function(y,h){
  y %>% ets(model="MAM", damped=TRUE) %>%
    forecast(h=h)
}

e6 <- tsCV(mytimeseries, etsfc, h=12)

MSE <- cbind(
  h=1:12,
  SNaive=colMeans(tail(e1, -14)^2, na.rm=TRUE), 
  Naive=colMeans(tail(e2, -14)^2, na.rm=TRUE), 
  drift=colMeans(tail(e3, -14)^2, na.rm=TRUE), 
  Mean=colMeans(tail(e4, -14)^2, na.rm=TRUE), 
  HW=colMeans(tail(e5, -14)^2, na.rm=TRUE), 
  ETS=colMeans(tail(e6, -14)^2, na.rm=TRUE)
)
MSE
```

## day 2

Transformations.  Box Cox

```{r}
elec %>% autoplot

(lambda <- BoxCox.lambda(elec))

elec %>%
  BoxCox(lambda) %>%
  autoplot

```

```{r}
fc <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80)
fc2 <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,
  biasadj=TRUE)
autoplot(eggs) +
  autolayer(fc, series="Simple back transformation") +
  autolayer(fc2, series="Bias adjusted", PI=FALSE) +
  guides(colour=guide_legend(title="Forecast"))

```

```{r}
lambda_adjusted <- function(data) {
  data %>% autoplot

  lambda <- BoxCox.lambda(data)
  print(lambda)
  
  print("with transform")
  data %>%
    BoxCox(lambda) %>%
    forecast %>%
    autoplot(subtitle = "with transform")
  
  print("without transform")
  data %>%
    forecast %>%
    autoplot(subtitle = "without transform")
}

data_list <- list(usnetelec,
                  mcopper,
                  enplanements,
                  a10
                  )
lapply(data_list, lambda_adjusted)

```

Rob uses lambda=0 quite a lot which is a log transform.

Canadian gas production
```{r}
cangas %>% autoplot

cangas %>%
    forecast %>%
    autoplot

cangas %>%
  ets %>%
  forecast %>%
  autoplot


```


```{r}
retaildata <- read.csv("https://robjhyndman.com/nyc2018/retail.csv")
mytimeseries <- ts(retaildata[,4], frequency=12, start=c(1982,4))

data <- mytimeseries
lambda <- BoxCox.lambda(data)
print(lambda)

data %>%
BoxCox(lambda) %>%
forecast %>%
autoplot
  
data %>%
forecast %>%
autoplot
  
```

trend
seasonal
cyclic

going to break things into trend/cycle and seasonal effects... then a remainder component

```{r}
elecequip %>%
  mstl %>%
  autoplot
```

```{r}
elecequip %>%
  autoplot  + 
  autolayer(trendcycle(mstl(elecequip)), series="Trend") 
  
```

```{r}
autoplot(elecequip, series="Data") +
autolayer(trendcycle(mstl(elecequip)), series="Trend") +
ylab("New orders index") + xlab("") +
ggtitle("Electrical equipment manufacturing (Euro area)")
```


trendcycle is loess smoother

```{r}

fit <- mstl(elecequip)

fit %>%
  trendcycle %>%
  autoplot

```

Decomp of the components using `mstl`:
```{r}
elecequip %>%
  mstl(s.window=5) %>%
  autoplot()

```

using two different `s.window` on the `mstl` function. Dark line is 25, colored line is 3. 
```{r}
elecequip %>%
  mstl(s.window=25) %>%
  autoplot() +
  autolayer(elecequip %>%
            mstl(s.window=3) )

```


* t.window controls wiggliness of trend component.
* s.window controls variation on seasonal component.
* seasonal() extracts seasonal component
* trendcycle() extracts trend component
* remainder() extracts remainder component
* seasadj() computes seasonally adjusted data

Checking the residuals is always a good idea. Look for patterns, spikes in the ACF mean cycles, and want histogram to be Gaussian(ish):

```{r}
bricksq %>%
  stlf(method="naive") %>%
  autoplot

bricksq %>%
  stlf(method="naive", lambda=.25, s.window='periodic') ->
  fitted_stlf

checkresiduals(fitted_stlf)
```



```{r}
bricksq %>%
  ets( ) ->
  fitted_stlf

checkresiduals(fitted_stlf)
autoplot(fitted_stlf)
```

```{r}
a10 %>%
  BoxCox.lambda() ->
  lambda

a10 %>%
  BoxCox(lambda) %>%
  auto.arima() ->
  fit

print(fit)
checkresiduals(fit, plot=TRUE)
fit %>% forecast() %>% autoplot()
```

```{r}
usgdp %>% 
  auto.arima %>%
  forecast(h=30, bootstrap=TRUE) %>%
  autoplot(include=20)

```

```{r}
trend <- seq_along(austa)
(fit1 <- auto.arima(austa, d=0, xreg=trend))
```

arima regression:

```{r}
fit <- auto.arima(y=uschange[,1], xreg=uschange[,2])
fit

checkresiduals(fit, test=FALSE)
```

electric demand

```{r}
qplot(elecdaily[,"Temperature"], elecdaily[,"Demand"]) +
xlab("Temperature") + ylab("Demand")

autoplot(elecdaily, facets = TRUE)

xreg <- cbind(MaxTemp = elecdaily[, "Temperature"],
  MaxTempSq = elecdaily[, "Temperature"]^2,
  Workday = elecdaily[, "WorkDay"])
fit <- auto.arima(elecdaily[, "Demand"], xreg = xreg)
checkresiduals(fit)

# Forecast one day ahead
forecast(fit, xreg = cbind(26, 26^2, 1))

fcast <- forecast(fit,
  xreg = cbind(rep(26,14), rep(26^2,14),
  c(0,1,0,0,1,1,1,1,1,0,0,1,1,1)))
autoplot(fcast) + ylab("Electicity demand (GW)")
```

what does it mean to "bring variables in as splines"?

piecewise linear trend:
```{r}
temp2 <- pmax(0, elecdaily[, "Temperature"]-20) # kink at 20 degrees

fit <- auto.arima(elecdaily[,"Demand"],
                  xreg=cbind(elecdaily[,"Temperature"], temp2, elecdaily[,"WorkDay"]))


```

```{r}
avecost <- motel[, "Takings"] / motel[,"Roomnights"]


```


```{r}
mytimeseries %>%
  BoxCox.lambda() ->
  lambda
plots <- list()
for (i in seq(6)) { 
  fit <- auto.arima(mytimeseries, 
                    seasonal=FALSE, 
                    lambda=0,
                    xreg=fourier(mytimeseries, K=i))
  
  print(fit$aicc)
  fit %>% forecast(fit, 
                   xreg=fourier(mytimeseries, K=i, h=14) ) %>%
    autoplot() +
    xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) -> 
    plots[[i]] 
  
}
```

```{r}
fit <- nnetar(bricksq, lambda=.25)
autoplot(forecast(fit,h=30))
```

```{r}
library(hts)
plot(infantgts)
smatrix(infantgts)

# Forecast 10-steps-ahead and reconcile the forecasts
infantforecast <- forecast(infantgts, h=10)

# Plot the forecasts including only the last ten historical years
plot(infantforecast, include=10)

# set up training and testing sets
training <- window(infantgts, end=1993)
test <- window(infantgts, start=1994)

# Compute forecasts on training data
forecast <- forecast(training, h=10)

# calculate ME, RMSE, MAE, MAPE, MPE and MASE
accuracy.gts(forecast, test)

```


## Fourier Transform example
straight out of the text book. Example of searching for the best K in the Fourier:
```{r}
cafe04 <- window(auscafe, start=2004)
plots <- list()
for (i in seq(6)) {
  fit <- auto.arima(cafe04, xreg = fourier(cafe04, K = i),
    seasonal = FALSE, lambda = 0)
  plots[[i]] <- autoplot(forecast(fit,
      xreg=fourier(cafe04, K=i, h=24))) +
    xlab(paste("K=",i,"   AICC=",round(fit[["aicc"]],2))) +
    ylab("") + ylim(1.5,4.7)
}
gridExtra::grid.arrange(
  plots[[1]],plots[[2]],plots[[3]],
  plots[[4]],plots[[5]],plots[[6]], nrow=3)
```

